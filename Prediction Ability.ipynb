{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5446e03a-719d-4b8c-93a7-ac13d7736446",
   "metadata": {},
   "source": [
    "## Dimitrios Fikos 9/12/2024\n",
    "##### Prediction Ability of competitor algorithms\n",
    "\n",
    "The capability of a dimension reduced dataset obtained by the first $d$ principal components, to predict accurately the response variable $Y$, is assessed. We are taking a dataset where $n=5000$ and $p=12$. This dataset is splitted into the training set and the test set. The model that we are working on here is mentioned and we expect $d=2$ as its true dimension. Gradients are computed by using $k=\\sqrt{n}$ and then SVD is applied to the training set. Then the first two vectors from the left singular matrix are selected. These are multiplied with the full data matrix in order to retrieve the dimension reduced dataset with dimensions ($n_{train},2$) and ($n_{test},2$) for training and test set respectively. This procedure is implemented for each of the proposed algorithms. Approach applied for the comparison is k nearest neighbors for regression. Here, we have $k=10$.  Six datasets in total are compared. The full dataset, $p=12$, the four dimension reduced datasets derived from the estimated projection vectors with $d=2$ and finally the dimension reduced dataset earned by the true projections $\\eta$ and $\\theta$ , again with $d=2$. The evaluation is made by the aid of Mean Squared Error (MSE) between the true response variable and the predicted one. Besides, the coefficient of determination, $R^2$ that measures the proportion of variance of the response variable explained, ranging from 0 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "297fc95a-14bb-4aa7-804a-bc9f7e94c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a10cc-6ed9-4ee9-9c93-52a699716f23",
   "metadata": {},
   "source": [
    "# Work with Model : $$g(x)=X_1 + X_2^2 +X_3$$ \n",
    "\n",
    " In this case\n",
    "$$ \\nabla g (x) = (1, 2X_2,1,0,\\ldots, 0) ^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a118ec9-8bc8-49ad-944b-a9688f535611",
   "metadata": {},
   "source": [
    "### Create 100 random points for all algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd1f4a1-d808-407a-bb01-9b8e6f79fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 100 random points N(0,1) with dimension p=12. \n",
    "rng=np.random.RandomState(42)\n",
    "x_specific_train = rng.normal(loc=0, scale=1, size=(100, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49876410-adfb-426b-af98-3cd473a2f8fe",
   "metadata": {},
   "source": [
    "### Make a random dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80de3d86-6726-4f8c-a0b4-8b3f4d065aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size n=5000 and p=12\n",
    "rng=np.random.RandomState(42)\n",
    "# reproducibility of data\n",
    "dataset=[]\n",
    "# Random variable X with 'dim' dimensions\n",
    "X = rng.normal(loc=0, scale=1, size=(5000, 12))\n",
    "# Make the errors\n",
    "errors = rng.normal(loc=0, scale=0.7, size=5000)\n",
    "# make the Y \n",
    "Y = X[:,0] + X[:, 1]**2 + X[:,2] + errors\n",
    "dataset.append((X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d3da1-8497-483c-a97b-4826e3006d07",
   "metadata": {},
   "source": [
    "## Train and test set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28702ac8-aff1-44b2-b735-5d1b66f86cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddabe1f-9b98-4912-8293-ffe192f1e395",
   "metadata": {},
   "source": [
    "## Knn to the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c7b6f-f253-423a-8c23-e46fc72dbdcb",
   "metadata": {},
   "source": [
    "### Writing the functions of all 4 gradient algorithms. \n",
    "\n",
    "#### 1) Gradient from linear regression b_ols\n",
    "\n",
    "#### 2) Gradient from linear regression b_ols + Lasso \n",
    "\n",
    "#### 3) Gradient deriving from diagonal of Gram matrix b_ols_diag\n",
    "\n",
    "#### 4) Gradient deriving from trace of Gram Matrix b_ols_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5472a048-027d-42a8-a4f2-21dea65629c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "####### 1) Projection from OLS ########\n",
    "def b_ols(x_specific3, X, Y, k, dim):\n",
    "    estimations=[]\n",
    "    # Number of neighbors\n",
    "    knn = NearestNeighbors(n_neighbors=k)\n",
    "    knn.fit(X)\n",
    "    for x in x_specific3:\n",
    "        x_spec = np.array([x])  \n",
    "        distances, indices = knn.kneighbors(x_spec)\n",
    "        # Get the k-nearest neighbors\n",
    "        nearest_X = X[indices].reshape(k, -1)\n",
    "        nearest_Y = Y[indices].flatten()\n",
    "        sum_xi = sum_yi =0 \n",
    "        for i in range(len(nearest_X)):\n",
    "            sum_xi+=nearest_X[i]\n",
    "            sum_yi+=nearest_Y[i]\n",
    "        sum_xi, sum_yi = sum_xi/k, sum_yi/k \n",
    "        nearest_X, nearest_Y = nearest_X - sum_xi, nearest_Y - sum_yi\n",
    "        # Initialize the Linear Regression model\n",
    "        linear_regression = LinearRegression(fit_intercept=False)\n",
    "        # Fit the model to the data\n",
    "        linear_regression.fit(nearest_X, nearest_Y)\n",
    "        # Print the coefficient(s)\n",
    "        estimated_gradient = linear_regression.coef_\n",
    "        estimations.append(estimated_gradient)\n",
    "    estimations=np.array(estimations).T\n",
    "    U, S, Vh =np.linalg.svd(estimations, full_matrices=True, compute_uv=True)\n",
    "    first_column = U[:, 0]\n",
    "    second_column = U[:, 1]\n",
    "    est_proj_1 = np.outer(first_column, first_column)\n",
    "    est_proj_2 = np.outer(second_column, second_column)\n",
    "    est_proj = est_proj_1 + est_proj_2\n",
    "    beta_real = np.zeros((dim, 1))\n",
    "    gamma_real=np.zeros((dim, 1))\n",
    "    beta_real[0, 0] =  gamma_real[1,0] = beta_real[2,0] =1 \n",
    "    beta_proj=beta_real@beta_real.T\n",
    "    beta_proj=beta_proj*1/2\n",
    "    gamma_proj=gamma_real@gamma_real.T\n",
    "    true_proj=beta_proj+gamma_proj\n",
    "    f_norm = np.linalg.norm(est_proj-true_proj, 'fro')\n",
    "    return f_norm, U, S, Vh\n",
    "\n",
    "\n",
    "####### 2) Projection deriving from OLS + Lasso ########\n",
    "def b_ols_lasso(x_specific3, X, Y, k, dim):\n",
    "    estimations=[]\n",
    "    # Number of neighbors\n",
    "    knn = NearestNeighbors(n_neighbors=k)\n",
    "    knn.fit(X)\n",
    "    for x in x_specific3:\n",
    "        x_spec = np.array([x])  \n",
    "        distances, indices = knn.kneighbors(x_spec)\n",
    "        # Get the k-nearest neighbors\n",
    "        nearest_X = X[indices].reshape(k, -1)\n",
    "        nearest_Y = Y[indices].flatten()\n",
    "        sum_xi = sum_yi =0 \n",
    "        for i in range(len(nearest_X)):\n",
    "            sum_xi+=nearest_X[i]\n",
    "            sum_yi+=nearest_Y[i]\n",
    "        sum_xi, sum_yi = sum_xi/k, sum_yi/k \n",
    "        nearest_X, nearest_Y = nearest_X - sum_xi, nearest_Y - sum_yi\n",
    "        # Initialize the Linear Regression model with lasso\n",
    "        lasso_model = LassoCV(alphas=alphas, cv=5, fit_intercept=False, max_iter=10000, tol=1e-4)\n",
    "        lasso_model.fit(nearest_X, nearest_Y)\n",
    "        # Print the coefficients\n",
    "        estimated_gradient = lasso_model.coef_\n",
    "        estimations.append(estimated_gradient)\n",
    "    estimations=np.array(estimations).T\n",
    "    U, S, Vh =np.linalg.svd(estimations, full_matrices=True, compute_uv=True)\n",
    "    first_column = U[:, 0]\n",
    "    second_column = U[:, 1]\n",
    "    est_proj_1 = np.outer(first_column, first_column)\n",
    "    est_proj_2 = np.outer(second_column, second_column)\n",
    "    est_proj = est_proj_1 + est_proj_2\n",
    "    beta_real = np.zeros((dim, 1))\n",
    "    gamma_real=np.zeros((dim, 1))\n",
    "    beta_real[0, 0] =  gamma_real[1,0] = beta_real[2,0] =1 \n",
    "    beta_proj=beta_real@beta_real.T\n",
    "    beta_proj=beta_proj*1/2\n",
    "    gamma_proj=gamma_real@gamma_real.T\n",
    "    true_proj=beta_proj+gamma_proj\n",
    "    f_norm = np.linalg.norm(est_proj-true_proj, 'fro')\n",
    "    return f_norm, U, S, Vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b49e201-8a2e-4949-b4c4-7837c36b963c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "####### 3) Projection deriving from OLS + diagonal ######## \n",
    "def b_ols_diag(x_specific3, X, Y, k, dim):\n",
    "    estimations=[]\n",
    "    # Number of neighbors\n",
    "    knn = NearestNeighbors(n_neighbors=k)\n",
    "    knn.fit(X)\n",
    "    for x in x_specific3:\n",
    "        x_spec = np.array([x])  \n",
    "        distances, indices = knn.kneighbors(x_spec)\n",
    "        # Get the k-nearest neighbors\n",
    "        nearest_X = X[indices].reshape(k, -1)\n",
    "        nearest_Y = Y[indices].flatten()\n",
    "        b_ols=[]\n",
    "        mean_X = np.mean(nearest_X, axis=0)\n",
    "        mean_Y = np.mean(nearest_Y)\n",
    "        for j in range(nearest_X.shape[1]):\n",
    "            numer = np.mean((nearest_X[:, j] - mean_X[j]) * (nearest_Y - mean_Y))\n",
    "            denom = np.mean((nearest_X[:, j] - mean_X[j]) ** 2)\n",
    "            b_ols.append(numer/denom)\n",
    "        estimations.append(b_ols)\n",
    "    estimations=np.array(estimations).T\n",
    "    U, S, Vh =np.linalg.svd(estimations, full_matrices=True, compute_uv=True)\n",
    "    first_column = U[:, 0]\n",
    "    second_column = U[:, 1]\n",
    "    est_proj_1 = np.outer(first_column, first_column)\n",
    "    est_proj_2 = np.outer(second_column, second_column)\n",
    "    est_proj = est_proj_1 + est_proj_2\n",
    "    beta_real = np.zeros((dim, 1))\n",
    "    gamma_real=np.zeros((dim, 1))\n",
    "    beta_real[0, 0] =  gamma_real[1,0] = beta_real[2,0] =1 \n",
    "    beta_proj=beta_real@beta_real.T\n",
    "    beta_proj=beta_proj*1/2\n",
    "    gamma_proj=gamma_real@gamma_real.T\n",
    "    true_proj=beta_proj+gamma_proj\n",
    "    f_norm = np.linalg.norm(est_proj-true_proj, 'fro')\n",
    "    return f_norm, U, S, Vh\n",
    "\n",
    "####### 4) Projection deriving from OLS + trace ######## \n",
    "def b_ols_trace(x_specific3, X, Y, k, dim):\n",
    "    estimations=[]\n",
    "    # Number of neighbors\n",
    "    knn = NearestNeighbors(n_neighbors=k)\n",
    "    knn.fit(X)\n",
    "    for x in x_specific3:\n",
    "        x_spec = np.array([x])  \n",
    "        distances, indices = knn.kneighbors(x_spec)\n",
    "        # Get the k-nearest neighbors\n",
    "        nearest_X = X[indices].reshape(k, -1)\n",
    "        nearest_Y = Y[indices].flatten()\n",
    "        b_ols=[]\n",
    "        mean_X = np.mean(nearest_X, axis=0)\n",
    "        mean_Y = np.mean(nearest_Y)\n",
    "        denom=0\n",
    "        for j in range(nearest_X.shape[1]):\n",
    "            numer=np.mean((nearest_X[:, j] - mean_X[j]) * (nearest_Y - mean_Y))\n",
    "            b_ols.append(numer)\n",
    "        estimations.append(b_ols)\n",
    "    estimations=np.array(estimations).T\n",
    "    U, S, Vh =np.linalg.svd(estimations, full_matrices=True, compute_uv=True)\n",
    "    first_column = U[:, 0]\n",
    "    second_column = U[:, 1]\n",
    "    est_proj_1 = np.outer(first_column, first_column)\n",
    "    est_proj_2 = np.outer(second_column, second_column)\n",
    "    est_proj = est_proj_1 + est_proj_2\n",
    "    beta_real = np.zeros((dim, 1))\n",
    "    gamma_real=np.zeros((dim, 1))\n",
    "    beta_real[0, 0] =  gamma_real[1,0] = beta_real[2,0] =1 \n",
    "    beta_proj=beta_real@beta_real.T\n",
    "    beta_proj=beta_proj*1/2\n",
    "    gamma_proj=gamma_real@gamma_real.T\n",
    "    true_proj=beta_proj+gamma_proj\n",
    "    f_norm = np.linalg.norm(est_proj-true_proj, 'fro')\n",
    "    return f_norm, U, S, Vh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb62e64-a49c-4f88-b1da-8b357b4693aa",
   "metadata": {},
   "source": [
    "## Compute the new X_train datasets (reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6fbe584-5e79-4f4b-9a6a-142df9ff1f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first two principal components\n",
    "num_components = 2\n",
    "# alphas for lasso grid\n",
    "alphas = np.logspace(-6, 0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fddb84-eb66-4eba-ad6b-d5b7dc83c3ba",
   "metadata": {},
   "source": [
    "### b_ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e09d8eff-74bd-4083-b189-8bdc8f8c92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_norm, U, S, Vh = b_ols(x_specific_train, X_train, Y_train, k=71, dim=12)  # Get singular values for this dataset\n",
    "# Select the top left singular vectors corresponding to these components\n",
    "top_ols= U[:,:num_components] \n",
    "# take the dimension reduced dataset\n",
    "X_train_new_ols = X_train@top_ols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab88ee-931e-415b-8a71-3a728578d13b",
   "metadata": {},
   "source": [
    "### b_ols_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cd23096-63a6-453d-922a-6e72ee7202bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_norm, U, S, Vh = b_ols_lasso(x_specific_train, X_train, Y_train, k=71, dim=12)\n",
    "# Select the top left singular vectors corresponding to these components\n",
    "top_lasso= U[:,:num_components]\n",
    "# take the dimension reduced dataset\n",
    "X_train_new_lasso = X_train@top_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b49d8ed-bf2b-42bb-891d-d79ee2fdf2d7",
   "metadata": {},
   "source": [
    "### b_ols_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dac8af9-1c62-402d-849d-e23d023adead",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_norm, U, S, Vh = b_ols_diag(x_specific_train, X_train, Y_train, k=71, dim=12)\n",
    "# Select the top left singular vectors corresponding to these components\n",
    "top_diag = U[:,:num_components]\n",
    "# take the dimension reduced dataset\n",
    "X_train_new_diag = X_train@top_diag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1fc4fa-f749-4c74-8b81-a58d36afdfe5",
   "metadata": {},
   "source": [
    "### b_ols_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5ac4dbc-dc4e-4c72-a052-10d88b4a1212",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_norm, U, S, Vh = b_ols_trace(x_specific_train, X_train, Y_train, k=71, dim=12)\n",
    "# Select the top left singular vectors corresponding to these components\n",
    "top_trace = U[:,:num_components]\n",
    "# take the dimension reduced dataset\n",
    "X_train_new_trace = X_train@top_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4eb31-a6fb-4855-8e90-6624f968ef06",
   "metadata": {},
   "source": [
    "## Dimension Reduction to test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48f21ec9-4685-40c9-b159-6d1fcb245527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the columns taken from the training set, compute the dimesnion reduced training set per each algorithm\n",
    "# b_ols\n",
    "X_test_new_ols = X_test@top_ols\n",
    "# b_ols_lasso\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7cce7cf-0d4b-45c8-8dd6-89c25214c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the top left singular vectors from training set, compute dimension reduced test set per algorithm\n",
    "# b_ols\n",
    "X_test_new_ols = X_test@top_ols\n",
    "# b_ols_lasso\n",
    "X_test_new_lasso = X_test@top_lasso\n",
    "# b_ols_diag\n",
    "X_test_new_diag = X_test@top_diag\n",
    "# b_ols_trace\n",
    "X_test_new_trace = X_test@top_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0878be2f-12ee-4975-8961-a3464d2208af",
   "metadata": {},
   "source": [
    "## Apply KNN regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b52ef4e8-8aec-43f6-8dcd-4bc4d1ee8f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.8707\n",
      "R^2 Score: 0.5741\n"
     ]
    }
   ],
   "source": [
    "# Train KNN in the full dataset\n",
    "knn = KNeighborsRegressor(n_neighbors=10, weights='uniform', metric='euclidean')\n",
    "knn.fit(X_train, Y_train)\n",
    "# Predictions\n",
    "Y_pred = knn.predict(X_test)\n",
    "# Evaluation\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "print(f\"R^2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdeca98-8acc-4440-ae1e-0fa69d24b48c",
   "metadata": {},
   "source": [
    "### b_ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f78feee-7336-4f35-8e92-844ce3013c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.5940\n",
      "R^2 Score: 0.8648\n"
     ]
    }
   ],
   "source": [
    "# Train KNN with b_ols reduced dataset\n",
    "knn = KNeighborsRegressor(n_neighbors=10, weights='uniform', metric='euclidean')\n",
    "knn.fit(X_train_new_ols, Y_train)\n",
    "#  Make predictions on the test set\n",
    "y_pred = knn.predict(X_test_new_ols)\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "r2 = r2_score(Y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3762419-fc9c-4b6e-960c-4378861453b3",
   "metadata": {},
   "source": [
    "### b_ols_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24c8fe9b-c018-4ade-8ce9-b30b8382db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.5922\n",
      "R^2 Score: 0.8652\n"
     ]
    }
   ],
   "source": [
    "# Train KNN with b_ols_lasso reduced dataset\n",
    "knn = KNeighborsRegressor(n_neighbors=10, weights='uniform', metric='euclidean')\n",
    "knn.fit(X_train_new_lasso, Y_train)\n",
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_test_new_lasso)\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "r2 = r2_score(Y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589a499-b83a-4cbb-978c-ef0df8f93cba",
   "metadata": {},
   "source": [
    "### b_ols_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a43920f-8591-4700-8e5c-4abe85222999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.6494\n",
      "R^2 Score: 0.8522\n"
     ]
    }
   ],
   "source": [
    "# Train KNN with b_ols_diag reduced dataset\n",
    "knn = KNeighborsRegressor(n_neighbors=10, weights='uniform', metric='euclidean')\n",
    "knn.fit(X_train_new_diag, Y_train)\n",
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_test_new_diag)\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "r2 = r2_score(Y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665162ce-d949-4645-82f2-2f34b45b3a50",
   "metadata": {},
   "source": [
    "### b_ols_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab3040be-9796-4992-871b-c9458142c1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.6288\n",
      "R^2 Score: 0.8569\n"
     ]
    }
   ],
   "source": [
    "# Train KNN with b_ols_trace reduced dataset \n",
    "knn = KNeighborsRegressor(n_neighbors=10, weights='uniform', metric='euclidean')\n",
    "knn.fit(X_train_new_trace, Y_train)\n",
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_test_new_trace)\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "r2 = r2_score(Y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615080ca-8c7f-4bb2-999c-93930b21b1f3",
   "metadata": {},
   "source": [
    "## Take advantage of $\\eta,\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fefdb63-41db-415c-a651-3e00fca40617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the model we are using we know the ideal vectors taken for dimension reduction\n",
    "eta=[1,0,1,0,0,0,0,0,0,0,0,0]\n",
    "theta=[0,1,0,0,0,0,0,0,0,0,0,0]\n",
    "array_U= np.column_stack((eta, theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3966ed4d-6dc4-4452-914a-6fa261f7d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New train and test datasets based on η and θ \n",
    "X_train_new= X_train@array_U\n",
    "X_test_new = X_test@array_U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b1efa0-9fa1-4e7b-83c6-7b978585bae3",
   "metadata": {},
   "source": [
    "## Application of knn regression to the true dimension reduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18b88899-60ca-46e1-83d5-a53a022b1fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.5965\n",
      "R^2 Score: 0.8642\n"
     ]
    }
   ],
   "source": [
    "# Train KNN \n",
    "knn = KNeighborsRegressor(n_neighbors=10, weights='uniform', metric='euclidean')\n",
    "knn.fit(X_train_new, Y_train)\n",
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_test_new)\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "r2 = r2_score(Y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46dfc94-2397-4dee-b8c3-e2e3d8e27d2b",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277c8cb-2942-4eaf-baf0-1eac1d29eecc",
   "metadata": {},
   "source": [
    "The full dataset shows the poorest performance with $MSE=1.87$ and $57\\%$ of the explained variability. All dimension reduced datasets outperform the full dataset indicating the importance of dimension reduction in the specific model. In particular, all $R^2$ coefficients are more than $28\\%$ higher than the full dataset and at the same time $MSE$ is lower. The true reduced dataset is obviously outperforming all other datasets and it is used in the experiment as a baseline, with the aim to see the four algorithms results reaching its performance and that seems to be the case here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
